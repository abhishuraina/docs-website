---
title: 'OpenTelemetry for Kubernetes'
tags:
    - Kubernetes integration
    - OpenTelemetry
metaDescription: "Learn how to monitor your Kubernetes Cluster using OpenTelemetry"
freshnessValidatedDate: 2024-07-23
---

<Callout title="preview">
  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy).
</Callout>

OpenTelemetry observability for Kubernetes provides complete, open-source setup paired with a top-notch Kubernetes UI that is already compatible with our proprietary Kubernetes instrumentation. Our Kubernetes UIs are designed to be provider agnostic, allowing you to select either OpenTelemetry or New Relic instrumentation based on your needs.

This document outlines the process for monitoring a Kubernetes cluster using OpenTelemetry. It involves the installation of the [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart within the cluster and the deployment of the necessary Collectors to enable first-class observability.

By integrating Kubernetes components into the OpenTelemetry Collector, we can transmit metrics, events, and logs directly to New Relic. These telemetry signals automatically enhance our out-of-the-box experiences such as the [Kubernetes Navigator](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#navigator-preview), [overview dashboard](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#cluster-overview-dashboard), [Kubernetes events](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#browse-your-kubernetes-events), or [Kubernetes APM summary page](/docs/apm/apm-ui-pages/monitoring/kubernetes-summary-page/).

## How it works? [#how-works]

The [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart deploys these OpenTelemetry Collectors:

* **Deamonset Collector**: Deployed on each worker node and responsible for gathering metrics from the underlying host in the node, the `cAdvisor`, the  `Kubelet`, and collecting logs from the containers.

* **Deployment collector**: Deployed on the control plane node and responsible for gathering metrics of Kube state metrics and Kubernetes cluster events.

  <img
    title="K8s OpenTelemetry diagram"
    alt="K8s OpenTelemetry diagram"
    src="/images/infrastructure_diagram_k8s-otel-stack.webp"
  />

## Requirements [#requirements]

To send Kubernetes telemetry data to New Relic, we need an OpenTelemetry Collector. Our New Relic distribution of OpenTelemetry (NRDOT) is already set up to automatically monitor a Kubernetes cluster. It does this by deploying all the necessary components through our [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart.

If you switch to a different OpenTelemetry Collector, make sure it has all the key components you need:

* [Attributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor)
* [Filter processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor)
* [Filelog receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
* [GroupByAttrs processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/groupbyattrsprocessor)
* [Hostmetrics receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver)
* [K8sAttributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor)
* [K8sevents receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8seventsreceiver)
* [Kubelet receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver)
* [MetricsTransform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor)
* [Prometheus receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver)
* [ResourceDetection processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor)
* [Resource processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
* [Transform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor)


<Callout variant="tip">

    Interested in using our Kubernetes OpenTelemetry observability, but don't want to install our Helm chart?

    1. Make sure that your Collector includes the previously highlighted components.
    2. Follow the setup instructions provided in this [document](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/collector.md) to configure your collector appropriately.

</Callout>


## Install your Kubernetes cluster with OpenTelemetry [#install]

To get OpenTelemetry up and running in your cluster, follow these steps:

1. Download the [Helm chart values file](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/values.yaml#L20-L24) adapt it to meet your specific requirements.

   * Cluster name and <InlinePopover type="licenseKey"/> are mandatory.

   * Check the entire list of [configuration parameters](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values).

2. Install the [Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) together with the values file.

   ```shell
   helm repo add newrelic https://helm-charts.newrelic.com
   helm upgrade nr-k8s-otel-collector newrelic/nr-k8s-otel-collector -f your-custom-values.yaml -n newrelic --create-namespace --install
   ```

3. Ensure the pods have been successfully spun up.

   ```shell
       kubectl get pods -n newrelic --watch
   ```

4. Make sure New Relic is getting the data it needs, including metrics, events, and logs, by running the right queries. See [Introduction to the query builder](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder/) for more information.

   ```sql
   FROM Metric SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
   FROM InfrastructureEvent SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
   FROM Log SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
   ```

5. If you're using a GKE AutoPilot cluster, it's necessary to apply the following configuration in your `values.yaml` file to ensure compatibility and proper functionality of the OpenTelemetry Collectors.

   ```yaml
   privileged: false
   receivers:
       filelog:
           enabled: false
   daemonset:
       containerSecurityContext:
           privileged: false
   ```

## Uninstall your Kubernetes cluster with OpenTelemetry [#uninstall]

To stop monitoring a Kubernetes cluster with OpenTelemetry, run this command:

```shell
    helm uninstall nr-k8s-otel-collector -n newrelic
```

## Reduce data ingest [#reduce-data-ingest]

The `LowDataMode` option is enabled by default to ingest only the metrics required by our Kubernetes UIs.

If you need to cut down even more on data ingestion, increase the scrape interval in the [`nr-k8s-otel-collector` chart values](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values) for each deployed component.

## Metrics [#metrics]

* [Metrics - Full list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-full.md)

* [Metrics - `LowDataMode` list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-lowDataMode.md)

## Find and use data [#find]

Check out these documents to learn more on how to find data:

* [Explore your Kubernetes cluster](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/) to know the status of your cluster, from the control plane to nodes and pods.

* [Kubernetes APM summary page](/docs/apm/apm-ui-pages/monitoring/kubernetes-summary-page/) which offers insights into your Kubernetes integration alongside your monitored applications.


## Mapping of Kubernetes OpenTelemetry metrics to New Relic integration metrics [#mapping]

New Relic provides a mapping of the Kubernetes OpenTelemetry metrics to the New Relic integration metrics. This mapping helps you understand the metrics and attributes that are available for monitoring your Kubernetes cluster.

<CollapserGroup>

    <Collapser 
    id="for-api-server" 
    title="For API Server"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `API Server` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "200" }}>OpenTelmetry Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Type: Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>apiserver_storage_objects</td>
        <td>apiserverStorageObjects_resource_RESOURCE-KIND</td>
        <td>K8sApiServerSample</td>
        <td>**Gauge:** Number of objects stored in the API server.</td>
        </tr>
        <tr>
        <td>go_goroutines</td>
        <td>goGoroutines</td>
        <td>K8sApiServerSample</td>
        <td>**Gauge:** Number of goroutines that currently exist.</td>
        </tr>
        <tr>
        <td>go_threads</td>
        <td>goThreads</td>
        <td>K8sApiServerSample, K8sControllerManagerSample, K8sEtcdSample, K8sSchedulerSample</td>
        <td>**Gauge:** Number of OS threads created.</td>
        </tr>
        <tr>
        <td>process_resident_memory_bytes</td>
        <td>processResidentMemoryBytes</td>
        <td>K8sApiServerSample</td>
        <td>**Gauge:** Resident memory size in bytes.</td>
        </tr>
    </tbody>
</table>

    </Collapser>

    <Collapser 
    id="for-cAdvisor" 
    title="For cAdvisor"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `cAdvisor` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "200px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>container_cpu_cfs_periods_total</td>
        <td>containerCpuCfsPeriodsTotal</td>
        <td>K8sContainerSample</td>
        <td>**Counter:** Total number of elapsed enforcement period intervals.</td>
        </tr>
        <tr>
        <td>container_cpu_cfs_throttled_periods_total</td>
        <td>containerCpuCfsThrottledPeriodsTotal</td>
        <td>K8sContainerSample</td>
        <td>**Counter:** Total number of throttled period intervals.</td>
        </tr>
        <tr>
        <td>container_cpu_usage_seconds_total</td>
        <td>cpuUsedCores</td>
        <td>K8sContainerSample</td>
        <td>**Counter:** Total CPU time consumed.</td>
        </tr>
        <tr>
        <td>container_memory_working_set_bytes</td>
        <td>memoryWorkingSetBytes</td>
        <td>K8sContainerSample</td>
        <td>**Gauge:** Working set size of memory in bytes.</td>
        </tr>
        <tr>
        <td>container_network_receive_bytes_total</td>
        <td>net.rxBytesPerSecond</td>
        <td>K8sPodSample</td>
        <td>**Counter:** Cumulative count of bytes received.</td>
        </tr>
        <tr>
        <td>container_network_receive_errors_total</td>
        <td>net.txBytesPerSecond</td>
        <td>memoryRequestedBytes</td>
        <td>**Counter:** Cumulative count of receive errors encountered.</td>
        </tr>
        <tr>
        <td>container_network_transmit_bytes_total</td>
        <td>net.errorsPerSecond</td>
        <td>K8sPodSample</td>
        <td>**Counter:** Cumulative count of bytes transmitted.</td>
        </tr>
        <tr>
        <td>container_network_transmit_errors_total</td>
        <td>net.errorsPerSecond</td>
        <td>K8sPodSample</td>
        <td>**Counter:** Cumulative count of transmit errors encountered.</td>
        </tr>
        <tr>
        <td>container_spec_memory_limit_bytes</td>
        <td>memoryLimitBytes</td>
        <td>K8sContainerSample</td>
        <td>**Gauge:** Memory limit of the container in bytes.</td>
        </tr>
    </tbody>
</table>

    </Collapser>


    <Collapser 
    id="for-controller-manager" 
    title="For controller manager"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `controller manager` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "150px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>go_goroutines</td>
        <td>goGoroutines</td>
        <td>K8sApiServerSample, K8sControllerManagerSample, K8sEtcdSample, K8sSchedulerSample</td>
        <td>**Gauge:** Number of goroutines that currently exist.</td>
        </tr>
        <tr>
        <td>process_resident_memory_bytes</td>
        <td>processResidentMemoryBytes</td>
        <td>K8sControllerManagerSample</td>
        <td>**Gauge:** Resident memory size in bytes.</td>
        </tr>
    </tbody>
</table>

    </Collapser>


    <Collapser 
    id="for-kubelet" 
    title="For kubelet"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `Kubelet` that has the `KubeletStats Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "200px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>container.cpu.utilization</td>
            <td>cpuCoresUtilization</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** CPU utilization percentage of the container.</td>
        </tr>
        <tr>
            <td>container.filesystem.capacity</td>
            <td>fsCapacityBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Total filesystem capacity for the container.</td>
        </tr>
        <tr>
            <td>container.filesystem.usage</td>
            <td>fsUsedBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Used filesystem space for the container.</td>
        </tr>
        <tr>
            <td>container.memory.usage</td>
            <td>memoryUsedBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Total memory usage of the container.</td>
        </tr>
        <tr>
            <td>go_goroutines</td>
            <td>goGoroutines</td>
            <td>K8sApiServerSample, K8sControllerManagerSample, K8sEtcdSample, K8sSchedulerSample</td>
            <td>**Gauge:** Number of goroutines that currently exist.</td>
        </tr>
        <tr>
            <td>go_threads</td>
            <td>goThreads</td>
            <td>K8sApiServerSample, K8sControllerManagerSample, K8sEtcdSample, K8sSchedulerSample</td>
            <td>**Gauge:** Number of OS threads created.</td>
        </tr>
        <tr>
            <td>k8s.node.cpu.time</td>
            <td>cpuUsedCoreMilliseconds</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Total CPU time used by the node.</td>
        </tr>
        <tr>
            <td>k8s.node.cpu.utilization</td>
            <td>allocatableCpuCoresUtilization</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** CPU utilization percentage of the node.</td>
        </tr>
        <tr>
            <td>k8s.node.filesystem.capacity</td>
            <td>fsCapacityBytes</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Total filesystem capacity for the node.</td>
        </tr>
        <tr>
            <td>k8s.node.filesystem.usage</td>
            <td>fsUsedBytes</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Used filesystem space for the node.</td>
        </tr>
        <tr>
            <td>k8s.node.memory.available</td>
            <td>memoryAvailableBytes</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Available memory for the node.</td>
        </tr>
        <tr>
            <td>k8s.node.memory.working_set</td>
            <td>memoryWorkingSetBytes</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Working set size of the node memory.</td>
        </tr>
        <tr>
            <td>k8s.pod.filesystem.available</td>
            <td>fsAvailableBytes</td>
            <td>K8sVolumeSample</td>
            <td>**Gauge:** Available filesystem space for the pod.</td>
        </tr>
        <tr>
            <td>k8s.pod.filesystem.capacity</td>
            <td>fsCapacityBytes</td>
            <td>K8sVolumeSample</td>
            <td>**Gauge:** Total filesystem capacity for the pod.</td>
        </tr>
        <tr>
            <td>k8s.pod.filesystem.usage</td>
            <td>fsUsedBytes</td>
            <td>K8sVolumeSample</td>
            <td>**Gauge:** Used filesystem space for the pod.</td>
        </tr>
        <tr>
            <td>k8s.pod.memory.working_set</td>
            <td>memoryWorkingSetBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Working set size of the pod memory.</td>
        </tr>
    </tbody>
</table>

The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `Kubelet` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "200px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>process_resident_memory_bytes</td>
            <td>processResidentMemoryBytes</td>
            <td>K8sApiServerSample</td>
            <td>**Gauge:** Resident memory size in bytes.</td>
        </tr>
    </tbody>
</table>

    </Collapser>

    <Collapser 
    id="for-kube-state-metrics" 
    title="For kube state metrics"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `KubeStateMetrics` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "200px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>kube_cronjob_created</td>
            <td>createdAt</td>
            <td>K8sCronjobSample</td>
            <td>**Gauge:** Creation timestamp of the CronJob.</td>
        </tr>
        <tr>
            <td>kube_cronjob_spec_suspend</td>
            <td>isSuspended</td>
            <td>K8sCronjobSample</td>
            <td>**Gauge:** Suspend flag of the CronJob.</td>
        </tr>
        <tr>
            <td>kube_cronjob_status_active</td>
            <td>isActive</td>
            <td>K8sCronjobSample</td>
            <td>**Gauge:** Number of active CronJob instances.</td>
        </tr>
        <tr>
            <td>kube_cronjob_status_last_schedule_time</td>
            <td>lastScheduledTime</td>
            <td>K8sCronjobSample</td>
            <td>**Gauge:** Last schedule time of the CronJob.</td>
        </tr>
        <tr>
            <td>kube_daemonset_created</td>
            <td>createdAt</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Creation timestamp of the DaemonSet.</td>
        </tr>
        <tr>
            <td>kube_daemonset_status_desired_number_scheduled</td>
            <td>podsScheduled</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Desired number of scheduled DaemonSet instances.</td>
        </tr>
        <tr>
            <td>kube_daemonset_status_number_misscheduled</td>
            <td>podsMisscheduled</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Number of misscheduled DaemonSet instances.</td>
        </tr>
        <tr>
            <td>kube_daemonset_status_number_ready</td>
            <td>podsReady</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Number of ready DaemonSet instances.</td>
        </tr>
        <tr>
            <td>kube_daemonset_status_number_unavailable</td>
            <td>podsUnavailable</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Number of unavailable DaemonSet instances.</td>
        </tr>
        <tr>
            <td>kube_daemonset_status_updated_number_scheduled</td>
            <td>podsUpdatedScheduled</td>
            <td>K8sDaemonsetSample</td>
            <td>**Gauge:** Updated number of scheduled DaemonSet instances.</td>
        </tr>
        <tr>
            <td>kube_deployment_created</td>
            <td>createdAt</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Creation timestamp of the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_metadata_generation</td>
            <td>metadataGeneration</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Generation number of the Deployment metadata.</td>
        </tr>
        <tr>
            <td>kube_deployment_spec_replicas</td>
            <td>podsDesired</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of desired replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_spec_strategy_rollingupdate_max_surge</td>
            <td>rollingUpdateMaxPodsSurge</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Maximum surge allowed during rolling update.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_condition</td>
            <td>conditionAvailable, conditionProgressing</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Deployment status conditions.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_observed_generation</td>
            <td>observedGeneration</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** The most recent generation observed for this Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_replicas</td>
            <td>podsTotal</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_replicas_available</td>
            <td>podsAvailable</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of available replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_replicas_ready</td>
            <td>podsReady</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of ready replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_replicas_unavailable</td>
            <td>podsUnavailable</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of unavailable replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_deployment_status_replicas_updated</td>
            <td>podsUpdated</td>
            <td>K8sDeploymentSample</td>
            <td>**Gauge:** Number of updated replicas for the Deployment.</td>
        </tr>
        <tr>
            <td>kube_horizontalpodautoscaler_spec_min_replicas</td>
            <td>minReplicas</td>
            <td>K8sHpaSample</td>
            <td>**Gauge:** Minimum number of replicas for the HorizontalPodAutoscaler.</td>
        </tr>
        <tr>
            <td>kube_horizontalpodautoscaler_status_condition</td>
            <td>isActive</td>
            <td>K8sHpaSample</td>
            <td>**Gauge:** Status conditions of the HorizontalPodAutoscaler.</td>
        </tr>
        <tr>
            <td>kube_horizontalpodautoscaler_status_current_replicas</td>
            <td>currentReplicas</td>
            <td>K8sHpaSample</td>
            <td>**Gauge:** Current number of replicas for the HorizontalPodAutoscaler.</td>
        </tr>
        <tr>
            <td>kube_horizontalpodautoscaler_status_desired_replicas</td>
            <td>desiredReplicas</td>
            <td>K8sHpaSample</td>
            <td>**Gauge:** Desired number of replicas for the HorizontalPodAutoscaler.</td>
        </tr>
        <tr>
            <td>kube_job_complete</td>
            <td>isComplete</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Whether the Job is complete `(1)` or not `(0)`.</td>
        </tr>
        <tr>
            <td>kube_job_created</td>
            <td>createdAt</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Creation timestamp of the Job.</td>
        </tr>
        <tr>
            <td>kube_job_failed</td>
            <td>failed</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Whether the Job has failed `(1)` or not `(0)`.</td>
        </tr>
        <tr>
            <td>kube_job_spec_active_deadline_seconds</td>
            <td>specActiveDeadlineSeconds</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Number of seconds the Job can run before being terminated.</td>
        </tr>
        <tr>
            <td>kube_job_spec_completions</td>
            <td>specCompletions</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Desired number of successfully finished pods for the Job.</td>
        </tr>
        <tr>
            <td>kube_job_spec_parallelism</td>
            <td>specParallelism</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Maximum desired number of pods executing in parallel for the Job.</td>
        </tr>
        <tr>
            <td>kube_job_status_active</td>
            <td>activePods</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Number of active pods for the Job.</td>
        </tr>
        <tr>
            <td>kube_job_status_completion_time</td>
            <td>completedAt</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Completion time of the Job.</td>
        </tr>
        <tr>
            <td>kube_job_status_failed</td>
            <td>failedPods</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Number of failed pods for the Job.</td>
        </tr>
        <tr>
            <td>kube_job_status_start_time</td>
            <td>startedAt</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Start time of the Job.</td>
        </tr>
        <tr>
            <td>kube_job_status_succeeded</td>
            <td>succeededPods</td>
            <td>K8sJobSample</td>
            <td>**Gauge:** Number of succeeded pods for the Job.</td>
        </tr>
        <tr>
            <td>kube_node_status_allocatable</td>
            <td>memoryWorkingSetUtilization</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Allocatable resources of the Node.</td>
        </tr>
        <tr>
            <td>kube_node_status_condition</td>
            <td>condition.CONDITION_NAME=CONDITION_VALUE</td>
            <td>K8sNodeSample</td>
            <td>**Gauge:** Condition of the Node's status.</td>
        </tr>
        <tr>
            <td>kube_persistentvolume_capacity_bytes</td>
            <td>capacityBytes</td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Capacity of the PersistentVolume in bytes.</td>
        </tr>
        <tr>
            <td>kube_persistentvolume_created</td>
            <td>createdAt</td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Creation timestamp of the PersistentVolume.</td>
        </tr>
        <tr>
            <td>kube_persistentvolume_info</td>
            <td></td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Information about the PersistentVolume.</td>
        </tr>
        <tr>
            <td>kube_persistentvolume_status_phase</td>
            <td>statusPhase</td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Phase of the PersistentVolume.</td>
        </tr>
        <tr>
            <td>kube_persistentvolumeclaim_created</td>
            <td>createdAt</td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Creation timestamp of the PersistentVolumeClaim.</td>
        </tr>
        <tr>
            <td>kube_persistentvolumeclaim_info</td>
            <td>All attributes describing the volume</td>
            <td>K8sPersistentVolumeSample</td>
            <td>**Gauge:** Information about the PersistentVolumeClaim.</td>
        </tr>
        <tr>
            <td>kube_persistentvolumeclaim_resource_requests_storage_bytes</td>
            <td>requestedStorageBytes</td>
            <td>K8sPersistentVolumeClaimSample</td>
            <td>**Gauge:** Storage resource requests of the PersistentVolumeClaim in bytes.</td>
        </tr>
        <tr>
            <td>kube_persistentvolumeclaim_status_phase</td>
            <td>statusPhase</td>
            <td>K8sPersistentVolumeClaimSample</td>
            <td>**Gauge:** Phase of the PersistentVolumeClaim.</td>
        </tr>
        <tr>
            <td>kube_pod_container_resource_limits</td>
            <td>cpuLimitCores, memoryLimitBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Resource limits of the Pod container.</td>
        </tr>
            <tr>
            <td>kube_pod_container_resource_requests</td>
            <td>cpuRequestedCores, memoryRequestedBytes</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Resource requests of the Pod container.</td>
        </tr>
        <tr>
            <td>kube_pod_container_status_phase</td>
            <td>status</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Current phase of the Pod container.</td>
        </tr>
        <tr>
            <td>kube_pod_container_status_ready</td>
            <td>isReady</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Whether the Pod container is ready (1) or not (0).</td>
        </tr>
        <tr>
            <td>kube_pod_container_status_restarts_total</td>
            <td>restartCount</td>
            <td>K8sContainerSample</td>
            <td>**Counter:** Total number of restarts for the Pod container.</td>
        </tr>
        <tr>
            <td>kube_pod_container_status_waiting_reason</td>
            <td>reason</td>
            <td>K8sContainerSample</td>
            <td>**Gauge:** Reason for the container waiting state.</td>
        </tr>
        <tr>
            <td>kube_pod_created</td>
            <td>createdAt</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Creation timestamp of the Pod.</td>
        </tr>
        <tr>
            <td>kube_pod_info</td>
            <td>All attributes describing the POD</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Information about the Pod.</td>
        </tr>
        <tr>
            <td>kube_pod_status_phase</td>
            <td>status</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Current phase of the Pod status.</td>
        </tr>
        <tr>
            <td>kube_pod_status_ready</td>
            <td>isReady</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Whether the Pod is ready (1) or not (0).</td>
        </tr>
        <tr>
            <td>kube_pod_status_ready_time</td>
            <td>startTime</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Time when the Pod status became ready.</td>
        </tr>
        <tr>
            <td>kube_pod_status_scheduled</td>
            <td>isScheduled</td>
            <td>K8sPodSample</td>
            <td>**Gauge:** Whether the Pod is scheduled (1) or not (0).</td>
        </tr>
        <tr>
            <td>kube_pod_status_scheduled_time</td>
            <td></td>
            <td></td>
            <td>**Gauge:** Time when the Pod became scheduled.</td>
        </tr>
        <tr>
            <td>kube_service_annotations</td>
            <td>selector.ANNOTATIONS</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Annotations applied to the Service.</td>
        </tr>
        <tr>
            <td>kube_service_created</td>
            <td>createdAt</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Creation timestamp of the Service.</td>
        </tr>
        <tr>
            <td>kube_service_info</td>
            <td>All attributes describing the Service</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Information about the Service.</td>
        </tr>
        <tr>
            <td>kube_service_labels</td>
            <td>label.LABEL_NAME</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Labels applied to the Service.</td>
        </tr>
        <tr>
            <td>kube_service_spec_type</td>
            <td>specType</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Type of the Service specification.</td>
        </tr>
        <tr>
            <td>kube_service_status_load_balancer_ingress</td>
            <td>filter with ```WHERE specType = 'LoadBalancer'```</td>
            <td>K8sServiceSample</td>
            <td>**Gauge:** Status of the load balancer ingress for the Service.</td>
        </tr>
        <tr>
            <td>kube_statefulset_created</td>
            <td>createdAt</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Creation timestamp of the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_persistentvolumeclaim_retention_policy</td>
            <td>filter with WHERE persistent = 'true'</td>
            <td>K8sVolumeSample</td>
            <td>**Gauge:** Retention policy of PersistentVolumeClaims for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_replicas</td>
            <td>podsDesired</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Desired number of replicas for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_current_revision</td>
            <td>currentRevision</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Current revision of the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_replicas</td>
            <td>podsTotal</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Number of replicas for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_replicas_available</td>
            <td>podsTotal - podsCurrent</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Number of available replicas for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_replicas_current</td>
            <td>podsCurrent</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Number of current replicas for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_replicas_ready</td>
            <td>podsReady</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Number of ready replicas for the StatefulSet.</td>
        </tr>
        <tr>
            <td>kube_statefulset_status_replicas_updated</td>
            <td>podsUpdated</td>
            <td>K8sStatefulsetSample</td>
            <td>**Gauge:** Number of updated replicas for the StatefulSet.</td>
        </tr>
    </tbody>
</table>

    </Collapser>

    <Collapser 
    id="k8s-otel" 
    title="For Node"
    >

The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `node` that has the `HostMetric Receiver`:


<table>
    <thead>
        <tr>
            <th style={{ width: "200px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr> 
            <td>process.cpu.utilization</td>
            <td>cpuPercent</td>
            <td>ProcessSample</td>
            <td>**Gauge:** CPU utilization of the process as a percentage.</td>
        </tr>
        <tr>
            <td>process.disk.io</td>
            <td>ioTotalReadCount+ioTotalWriteCount</td>
            <td>ProcessSample</td>
            <td>**Counter:** Number of disk I/O operations performed by the process.</td>
        </tr>
        <tr>
            <td>process.memory.usage</td>
            <td>memoryResidentSizeBytes</td>
            <td>ProcessSample</td>
            <td>**Gauge:** Memory usage of the process in bytes.</td>
        </tr>
        <tr>
            <td>process.memory.virtual</td>
            <td>memoryVirtualSizeBytes</td>
            <td>ProcessSample</td>
            <td>**Gauge:** Virtual memory usage of the process in bytes.</td>
        </tr>
        <tr>
            <td>system.cpu.load_average.15m</td>
            <td>loadAverageFifteenMinute</td>
            <td>SystemSample</td>
            <td>**Gauge:** System load average over the last 15 minutes.</td>
        </tr>
        <tr>
            <td>system.cpu.load_average.1m</td>
            <td>loadAverageOneMinute</td>
            <td>SystemSample</td>
            <td>**Gauge:** System load average over the last 1 minute.</td>
        </tr>
        <tr>
            <td>system.cpu.load_average.5m</td>
            <td>loadAverageFiveMinute</td>
            <td>SystemSample</td>
            <td>**Gauge:** System load average over the last 5 minutes.</td>
        </tr>
        <tr>
            <td>system.cpu.utilization</td>
            <td>cpuPercent</td>
            <td>SystemSample</td>
            <td>**Gauge:** Total CPU utilization percentage.</td>
        </tr>
        <tr>
            <td>system.disk.io</td>
            <td>readBytesPerSecond+writeBytesPerSecond</td>
            <td>StorageSample</td>
            <td>**Counter:** Number of disk I/O operations performed.</td>
        </tr>
        <tr>
            <td>system.disk.io_time</td>
            <td>diskReadsPerSecond+diskWritesPerSecond</td>
            <td>SystemSample</td>
            <td>**Counter:** Time spent in disk I/O operations in seconds.</td>
        </tr>
        <tr>
            <td>system.disk.operation_time</td>
            <td>diskWriteUtilizationPercent</td>
            <td>SystemSample</td>
            <td>**Counter:** Total time spent in disk operations in seconds.</td>
        </tr>
        <tr>
            <td>system.disk.operations</td>
            <td>readIoPerSecond</td>
            <td>StorageSample</td>
            <td>**Counter:** Number of disk operations performed.</td>
        </tr>
        <tr>
            <td>system.filesystem.usage</td>
            <td>diskUsedBytes</td>
            <td>SystemSample</td>
            <td>**Gauge:** Usage of filesystem space in bytes.</td>
        </tr>
        <tr>
            <td>system.filesystem.utilization</td>
            <td>diskUsedPercent</td>
            <td>SystemSample</td>
            <td>**Gauge:** Utilization of the filesystem as a percentage.</td>
        </tr>
        <tr>
            <td>system.memory.usage</td>
            <td>memoryUsedBytes</td>
            <td>SystemSample</td>
            <td>**Gauge:** Total memory usage in bytes.</td>
        </tr>
        <tr>
            <td>system.memory.utilization</td>
            <td>memoryUsedPercent</td>
            <td>SystemSample</td>
            <td>**Gauge:** Memory utilization as a percentage.</td>
        </tr>
        <tr>
            <td>system.network.errors</td>
            <td>transmitErrorsPerSecond</td>
            <td>NetworkSample</td>
            <td>**Counter:** Number of network errors.</td>
        </tr>
        <tr>
            <td>system.network.io</td>
            <td>receiveBytesPerSecond, transmitBytesPerSecond</td>
            <td>NetworkSample</td>
            <td>**Counter:** Number of network I/O operations.</td>
        </tr>
        <tr>
            <td>system.network.packets</td>
            <td>transmitPacketsPerSecond+receivePacketsPerSecond</td>
            <td>NetworkSample</td>
            <td>**Counter:** Number of network packets transmitted and received.</td>
        </tr>

    </tbody>
</table>


    </Collapser>

    <Collapser 
    id="for-scheduler" 
    title="For Scheduler"
    >

    The following table maps the Kubernetes OpenTelemetry metrics to the New Relic integration metrics for the `scheduler` that has the `Prometheus Receiver`:

<table>
    <thead>
        <tr>
            <th style={{ width: "150px" }}>Metric</th>
            <th>Attribute name</th>
            <th>New Relic Event Name</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>go_goroutines</td>
            <td>goGoroutines</td>
            <td>K8sApiServerSample, K8sControllerManagerSample, K8sEtcdSample, K8sSchedulerSample</td>
            <td>**Gauge:** Number of goroutines that currently exist.</td>
        </tr>
        <tr>
            <td>process_resident_memory_bytes</td>
            <td>processResidentMemoryBytes</td>
            <td>K8sSchedulerSample</td>
            <td>**Gauge:** Resident memory size in bytes.</td>
        </tr>
    </tbody>
</table>

    </Collapser>


</CollapserGroup>

## Troubleshooting [#troubleshooting]

Check out the logs of the Collector pod that's experiencing issues. Run this command:

```shell
    kubectl logs <otel-pod-name> -n newrelic
```

You can also set the `verboseLog` parameter to `true` in the [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values) Helm chart.

## Common errors [#common-erros]

Check out the [Common errors section](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#common-errors) in our GitHub repository.

## Support [#support]

If you have issues with the OpenTelemetry observability for Kubernetes:

* Have a look at the [issues section on GitHub](https://github.com/newrelic/helm-charts/issues) for any similar problems or consider opening a new issue.
